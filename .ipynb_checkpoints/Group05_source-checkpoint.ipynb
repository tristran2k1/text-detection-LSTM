{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b9ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830326b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the project gutenberg ebook of the jungle book, by rudyard kipling\n",
      "\n",
      "this ebook is for the use of an\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "filename = \"./jungle_book.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "print(raw_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec6b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT : remove number\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a1849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text))) #List of every character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c0de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of characters mapped to integer values\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "#Do the reverse so we can print our predictions in characters and not integers\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1257ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the text; corpus length:  292870\n",
      "Total Vocab:  51\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f90abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 29281\n"
     ]
    }
   ],
   "source": [
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10 \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values: next value follow X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1e57a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29281, 60, 51)\n",
      "(29281, 51)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False  True False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False]]\n"
     ]
    }
   ],
   "source": [
    "#Vectorization returns a vector for all sentences indicating the presence or absence of a character. \n",
    "\n",
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c916f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               92160     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 51)                6579      \n",
      "=================================================================\n",
      "Total params: 98,739\n",
      "Trainable params: 98,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247db3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68eaf905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 2.4924\n",
      "Epoch 00001: loss improved from inf to 2.49238, saving model to saved_weights\\saved_weights-01-2.4924.hdf5\n",
      "229/229 [==============================] - 23s 100ms/step - loss: 2.4924\n",
      "Epoch 2/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 2.0244\n",
      "Epoch 00002: loss improved from 2.49238 to 2.02439, saving model to saved_weights\\saved_weights-02-2.0244.hdf5\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 2.0244\n",
      "Epoch 3/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.8320\n",
      "Epoch 00003: loss improved from 2.02439 to 1.83197, saving model to saved_weights\\saved_weights-03-1.8320.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.8320\n",
      "Epoch 4/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.6902\n",
      "Epoch 00004: loss improved from 1.83197 to 1.69021, saving model to saved_weights\\saved_weights-04-1.6902.hdf5\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 1.6902\n",
      "Epoch 5/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.5693\n",
      "Epoch 00005: loss improved from 1.69021 to 1.56993, saving model to saved_weights\\saved_weights-05-1.5699.hdf5\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 1.5699\n",
      "Epoch 6/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.4608\n",
      "Epoch 00006: loss improved from 1.56993 to 1.46076, saving model to saved_weights\\saved_weights-06-1.4608.hdf5\n",
      "229/229 [==============================] - 19s 84ms/step - loss: 1.4608\n",
      "Epoch 7/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.3630\n",
      "Epoch 00007: loss improved from 1.46076 to 1.36302, saving model to saved_weights\\saved_weights-07-1.3630.hdf5\n",
      "229/229 [==============================] - 18s 80ms/step - loss: 1.3630\n",
      "Epoch 8/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.2826\n",
      "Epoch 00008: loss improved from 1.36302 to 1.28256, saving model to saved_weights\\saved_weights-08-1.2826.hdf5\n",
      "229/229 [==============================] - 19s 81ms/step - loss: 1.2826\n",
      "Epoch 9/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2108\n",
      "Epoch 00009: loss improved from 1.28256 to 1.21105, saving model to saved_weights\\saved_weights-09-1.2110.hdf5\n",
      "229/229 [==============================] - 20s 87ms/step - loss: 1.2110\n",
      "Epoch 10/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.1561\n",
      "Epoch 00010: loss improved from 1.21105 to 1.15611, saving model to saved_weights\\saved_weights-10-1.1561.hdf5\n",
      "229/229 [==============================] - 16s 69ms/step - loss: 1.1561\n",
      "Epoch 11/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1099\n",
      "Epoch 00011: loss improved from 1.15611 to 1.11053, saving model to saved_weights\\saved_weights-11-1.1105.hdf5\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 1.1105\n",
      "Epoch 12/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0750\n",
      "Epoch 00012: loss improved from 1.11053 to 1.07499, saving model to saved_weights\\saved_weights-12-1.0750.hdf5\n",
      "229/229 [==============================] - 15s 67ms/step - loss: 1.0750\n",
      "Epoch 13/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0452\n",
      "Epoch 00013: loss improved from 1.07499 to 1.04521, saving model to saved_weights\\saved_weights-13-1.0452.hdf5\n",
      "229/229 [==============================] - 18s 80ms/step - loss: 1.0452\n",
      "Epoch 14/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0235\n",
      "Epoch 00014: loss improved from 1.04521 to 1.02352, saving model to saved_weights\\saved_weights-14-1.0235.hdf5\n",
      "229/229 [==============================] - 24s 104ms/step - loss: 1.0235\n",
      "Epoch 15/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0008\n",
      "Epoch 00015: loss improved from 1.02352 to 1.00081, saving model to saved_weights\\saved_weights-15-1.0008.hdf5\n",
      "229/229 [==============================] - 19s 82ms/step - loss: 1.0008\n",
      "Epoch 16/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00016: loss improved from 1.00081 to 0.98731, saving model to saved_weights\\saved_weights-16-0.9873.hdf5\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.9873\n",
      "Epoch 17/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9584\n",
      "Epoch 00017: loss improved from 0.98731 to 0.95841, saving model to saved_weights\\saved_weights-17-0.9584.hdf5\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 0.9584\n",
      "Epoch 18/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9445\n",
      "Epoch 00018: loss improved from 0.95841 to 0.94452, saving model to saved_weights\\saved_weights-18-0.9445.hdf5\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 0.9445\n",
      "Epoch 19/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9191\n",
      "Epoch 00019: loss improved from 0.94452 to 0.91915, saving model to saved_weights\\saved_weights-19-0.9191.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.9191\n",
      "Epoch 20/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.9153\n",
      "Epoch 00020: loss improved from 0.91915 to 0.91567, saving model to saved_weights\\saved_weights-20-0.9157.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9157\n",
      "Epoch 21/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8930\n",
      "Epoch 00021: loss improved from 0.91567 to 0.89303, saving model to saved_weights\\saved_weights-21-0.8930.hdf5\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.8930\n",
      "Epoch 22/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8905\n",
      "Epoch 00022: loss improved from 0.89303 to 0.89052, saving model to saved_weights\\saved_weights-22-0.8905.hdf5\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.8905\n",
      "Epoch 23/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.8693\n",
      "Epoch 00023: loss improved from 0.89052 to 0.86988, saving model to saved_weights\\saved_weights-23-0.8699.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.8699\n",
      "Epoch 24/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8633\n",
      "Epoch 00024: loss improved from 0.86988 to 0.86331, saving model to saved_weights\\saved_weights-24-0.8633.hdf5\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 0.8633\n",
      "Epoch 25/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8467- ETA: \n",
      "Epoch 00025: loss improved from 0.86331 to 0.84673, saving model to saved_weights\\saved_weights-25-0.8467.hdf5\n",
      "229/229 [==============================] - 23s 100ms/step - loss: 0.8467\n",
      "Epoch 26/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8365\n",
      "Epoch 00026: loss improved from 0.84673 to 0.83651, saving model to saved_weights\\saved_weights-26-0.8365.hdf5\n",
      "229/229 [==============================] - 20s 87ms/step - loss: 0.8365\n",
      "Epoch 27/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8165\n",
      "Epoch 00027: loss improved from 0.83651 to 0.81650, saving model to saved_weights\\saved_weights-27-0.8165.hdf5\n",
      "229/229 [==============================] - 21s 91ms/step - loss: 0.8165\n",
      "Epoch 28/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8072\n",
      "Epoch 00028: loss improved from 0.81650 to 0.80720, saving model to saved_weights\\saved_weights-28-0.8072.hdf5\n",
      "229/229 [==============================] - 19s 83ms/step - loss: 0.8072\n",
      "Epoch 29/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.7959\n",
      "Epoch 00029: loss improved from 0.80720 to 0.79623, saving model to saved_weights\\saved_weights-29-0.7962.hdf5\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 0.7962\n",
      "Epoch 30/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7835\n",
      "Epoch 00030: loss improved from 0.79623 to 0.78345, saving model to saved_weights\\saved_weights-30-0.7835.hdf5\n",
      "229/229 [==============================] - 16s 71ms/step - loss: 0.7835\n",
      "Epoch 31/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7753\n",
      "Epoch 00031: loss improved from 0.78345 to 0.77533, saving model to saved_weights\\saved_weights-31-0.7753.hdf5\n",
      "229/229 [==============================] - 25s 109ms/step - loss: 0.7753\n",
      "Epoch 32/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7627\n",
      "Epoch 00032: loss improved from 0.77533 to 0.76270, saving model to saved_weights\\saved_weights-32-0.7627.hdf5\n",
      "229/229 [==============================] - 19s 83ms/step - loss: 0.7627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7588\n",
      "Epoch 00033: loss improved from 0.76270 to 0.75881, saving model to saved_weights\\saved_weights-33-0.7588.hdf5\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 0.7588\n",
      "Epoch 34/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7534\n",
      "Epoch 00034: loss improved from 0.75881 to 0.75337, saving model to saved_weights\\saved_weights-34-0.7534.hdf5\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 0.7534\n",
      "Epoch 35/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7477\n",
      "Epoch 00035: loss improved from 0.75337 to 0.74774, saving model to saved_weights\\saved_weights-35-0.7477.hdf5\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 0.7477\n",
      "Epoch 36/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.7390\n",
      "Epoch 00036: loss improved from 0.74774 to 0.73843, saving model to saved_weights\\saved_weights-36-0.7384.hdf5\n",
      "229/229 [==============================] - 22s 94ms/step - loss: 0.7384\n",
      "Epoch 37/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.7263\n",
      "Epoch 00037: loss improved from 0.73843 to 0.72730, saving model to saved_weights\\saved_weights-37-0.7273.hdf5\n",
      "229/229 [==============================] - 16s 70ms/step - loss: 0.7273\n",
      "Epoch 38/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7142\n",
      "Epoch 00038: loss improved from 0.72730 to 0.71423, saving model to saved_weights\\saved_weights-38-0.7142.hdf5\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 0.7142\n",
      "Epoch 39/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.7062\n",
      "Epoch 00039: loss improved from 0.71423 to 0.70602, saving model to saved_weights\\saved_weights-39-0.7060.hdf5\n",
      "229/229 [==============================] - 21s 94ms/step - loss: 0.7060\n",
      "Epoch 40/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.6999\n",
      "Epoch 00040: loss improved from 0.70602 to 0.70098, saving model to saved_weights\\saved_weights-40-0.7010.hdf5\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.7010\n",
      "Epoch 41/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6983\n",
      "Epoch 00041: loss improved from 0.70098 to 0.69828, saving model to saved_weights\\saved_weights-41-0.6983.hdf5\n",
      "229/229 [==============================] - 20s 87ms/step - loss: 0.6983\n",
      "Epoch 42/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6906\n",
      "Epoch 00042: loss improved from 0.69828 to 0.69065, saving model to saved_weights\\saved_weights-42-0.6906.hdf5\n",
      "229/229 [==============================] - 22s 97ms/step - loss: 0.6906\n",
      "Epoch 43/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6822\n",
      "Epoch 00043: loss improved from 0.69065 to 0.68220, saving model to saved_weights\\saved_weights-43-0.6822.hdf5\n",
      "229/229 [==============================] - 19s 84ms/step - loss: 0.6822\n",
      "Epoch 44/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6749\n",
      "Epoch 00044: loss improved from 0.68220 to 0.67492, saving model to saved_weights\\saved_weights-44-0.6749.hdf5\n",
      "229/229 [==============================] - 24s 106ms/step - loss: 0.6749\n",
      "Epoch 45/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6606\n",
      "Epoch 00045: loss improved from 0.67492 to 0.66062, saving model to saved_weights\\saved_weights-45-0.6606.hdf5\n",
      "229/229 [==============================] - 21s 93ms/step - loss: 0.6606\n",
      "Epoch 46/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6531\n",
      "Epoch 00046: loss improved from 0.66062 to 0.65309, saving model to saved_weights\\saved_weights-46-0.6531.hdf5\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 0.6531\n",
      "Epoch 47/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6494\n",
      "Epoch 00047: loss improved from 0.65309 to 0.64942, saving model to saved_weights\\saved_weights-47-0.6494.hdf5\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 0.6494\n",
      "Epoch 48/50\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.6387\n",
      "Epoch 00048: loss improved from 0.64942 to 0.63932, saving model to saved_weights\\saved_weights-48-0.6393.hdf5\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 0.6393\n",
      "Epoch 49/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6285\n",
      "Epoch 00049: loss improved from 0.63932 to 0.62848, saving model to saved_weights\\saved_weights-49-0.6285.hdf5\n",
      "229/229 [==============================] - 18s 80ms/step - loss: 0.6285\n",
      "Epoch 50/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6233\n",
      "Epoch 00050: loss improved from 0.62848 to 0.62328, saving model to saved_weights\\saved_weights-50-0.6233.hdf5\n",
      "229/229 [==============================] - 24s 106ms/step - loss: 0.6233\n"
     ]
    }
   ],
   "source": [
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748acd73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18880/836313412.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#plot the training and validation accuracy and loss at each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aea7b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate character\n",
    "def GenerateCharacter(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b947058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed for our text prediction: \"year in the big gale.”\n",
      "\n",
      "“i’m not going near him,” said patal\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18880/1927488921.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mnext_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnext_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)\n",
    "\n",
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "\n",
    "\n",
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e91acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
